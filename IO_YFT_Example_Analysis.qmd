---
title: "Indian Ocean Yellowfin fishblicc Analysis"
author: "Paul Medley"
date:   "August 2023"
format: pdf
toc: true
keep_md: true
editor: source
number-sections: true
font:
  fontsize: 11
execute:
  echo: false
---

```{r setup}
#| include: false
library("here")
library("ggplot2")
library("dplyr")
library("tidyr")
#library("fishblicc")
library("devtools")
library("rstantools")
library("flextable")

sensitivities <- tibble()
add_sensitivity <- function(res, notes) {
  with(res$rp_df,
       rbind(
         sensitivities,
         tibble(
           model_name = res$ld$model_name,
           Linf = Linf,
           Galpha = Galpha,
           Mk = Mk,
           SPR = SPR,
           lp = lp__,
           Notes = notes
         )
       ))
}

load_all()
```


# Summary

This is an example analysis of Indian Ocean yellowfin tuna (*Thunnus albacares*) public IOTC data using the Bayesian length-based catch curve `fishblicc` which can model fisheries with length frequency data from multiple gears with different selectivities. The selectivity functions are flexible so that they can account for gears targeting different sizes of fish. The model estimates fishing mortality in each length bin for each gear and the spawning potential ratio (SPR) for the population. This assessment using only length frequency data estimated an SPR of around 16-34% (80% CI) for 2013-2019 compared to the estimate of SB/SB~0~ for the full stock assessment in 2020 of 24-38% (80% CI). 

# Introduction

This is an example analysis of Indian Ocean yellowfin tuna (*Thunnus albacares*). This stock undergoes regular Stock Synthesis (SS3) stock assessments which make use of all available data and undergoes rigorous review through the IOTC science committee. Part of the available data reported by countries that are members of IOTC include length frequency and catch data from a variety of gears, which are suitable for a multi-selectivity catch curve. This is a case study to see how the method compares with the full assessment and consider the problems that arise when dealing with limited data. There is no intention here to provide a competing stock assessment since important informative data that are available for analysis are actively excluded here.  

Catch curves are a simple idea - they use a snapshot of the age structure to infer the stock status and fishing mortality. This is made more complicated when dealing with lengths as a growth model is required to link length to age. But in principle the length-based catch curve applies the same idea. Is the proportion of larger mature fish in the landings consistent with the stock being sustainably harvested or not?

The main problem with length-based catch curve models has been that model flexibility has been limited. Specifically, they have not been able to deal with alternative selectivity functions that might be considered plausible, growth variability or alternative mortality models. The analysis below illustrates the `fishblicc` package which applies a more flexible model that estimates a fixed mortality within each length interval, so it is able to account for different selectivity curves and the resulting different mortality schedules that will change the length composition in the underlying population. The model also includes explicit growth variability.

The Indian Ocean yellowfin tuna fishery is probably more complex than most fisheries in terms of the gears used, but it is not unusual for a fishery, and particularly a small scale fishery, to have several gears operating. Having each gear separately modelled can produce much better results than trying to combine the data into single frequency samples or simply ignoring gears with lower catches. It will also allow gear-specific management advice when limiting fishing mortality and trying to improve overall fishing selectivity.

# Data

## Length Frequency Data

The data used are public length frequency data from \[www.iotc.org\] and is the same as that used in the 2019 SS3 yellowfin tuna stock assessment. The data were combined over 6 years (2013-2019) for this analysis (i.e. the last six years used in the SS3 assessment). The data were rescaled to avoid numerical overflow however[^1]. 

[^1]: Sampling size compositions from purse seine has been particularly high compared to other gears.

The recognised gears reflect the available length frequency data:

- BB: Bait boats (pole and line)

- GI: gill net

- HD: handline

- LL: longline

- OT: "other" but probably mostly troll fisheries

- PS-FS: Purse seine free school

- PS-LS: Purse seine log school, but mostly sets on fish aggregation devices (FADs)

These gears cover the vast majority of the known catch of yellowfin in the Indian Ocean (@fig-catch_data; @fig-length_frequency_data).  


```{r}
#| output: false
load(here("data", "iotc_yft_2013_19.rda"))
```


```{r }
#| label: fig-catch_data
#| fig-cap: "Total catch weight for 2013-2019 by gear type."  
ggplot(yft_catch_2013_19, aes(x=gear, y=catch)) +
  geom_col(fill="green", alpha=0.5) +
  labs(y = "Catch (t)", x = "Gear") +
  theme_classic()

```

```{r }
#| output: false
lfd <- yft_lenfreq_2013_19 |>
  pivot_longer(cols=BB:`PS-LS`, names_to="gear", values_to="fq") |>
  group_by(gear) |>
  mutate(Proportion=fq/sum(fq)) |>
  ungroup()
```


```{r }
#| label: fig-length_frequency_data
#| fig-cap: "Length frequency data as proportions for each gear."  
lfd |>
  ggplot(aes(x=Len, y=Proportion)) +
  geom_col(fill="lightblue") +
  facet_wrap(vars(gear), ncol=4) +
  labs(x = "Length (cm)", y = "Proportion") +
  theme_classic()
```
  
## Catch Numbers

For multiple gears, the relative catch in numbers for each gear is required to estimate the relative fishing mortality each gear applies. For data limited assessments, these relative catch data are potentially difficult to estimate. In many cases, the relative sample size of length frequency data taken for each gear may be the best indicator of the relative catches. If estimates of catches are available, these should be used. If necessary, proxies might also be used such as relative fleet sizes with CPUE data for example. As only relative catch sizes are needed, not absolute estimates, it may be possible to obtain them from more innovative approaches.

For IO yellowfin tuna, direct estimates of catch weight are available (@fig-catch_numbers), and these have been converted to numbers using the mean weights from the length frequency samples (@fig-catch_numbers). The length-weight conversion is estimated slightly differently between longline and the other gears. This alternative length-weight relationship was applied to longline and handline, which catch similar size fish.

It would be possible to fit a catch curve to the combined length frequencies as long as they are weighted by the relative catch numbers (@fig-cum_wt_prop_lfd). While it would be possible to fit a single catch curve to these combined frequency data, it would still be necessary to have a multi-modal function to model the selectivity. Perhaps more importantly, combining the data in this way would limit the advice that can be provided on a gear-by-gear basis. It would make it difficult, for example, to predict the effect of limiting the number of FADs or gillnets. 

```{r }

# Generate mean weights
a <- 0.00002459
b <- 2.9667

LMP <- seq(12, 202, by=4)
wt_sg <- a * LMP^b                       # all gears except
wt_ll <- 1.13*0.0000094007 * LMP^3.126843987 # longline/handline

gr <- pull(yft_catch_2013_19, gear)
mwt <- double(length(gr))
i <- 1
for (g in gr) {
  c1 <- enquo(g)
  fq <- pull(yft_lenfreq_2013_19, {{c1}})
  mwt[i] <- if_else(g %in% c("LL", "HD"), 
                    sum(fq * wt_ll)/sum(fq), 
                    sum(fq * wt_sg)/sum(fq)) 
  i <- i+1L
}

# catch in t, mwt in kilos, cn in thousands of fish
yft_catch_2013_19$catch_n <- yft_catch_2013_19$catch/mwt
```



```{r }
#| label: fig-catch_numbers
#| fig-cap: "Estimated catch numbers 2013-2019 for each gear."
ggplot(yft_catch_2013_19, aes(x=gear, y=catch_n)) +
  geom_col(fill="lightblue") +
  theme_classic() +
  labs(y = "Catch ('000s fish)", x = "Gear type")
```


```{r }
cn <- pull(yft_catch_2013_19, catch_n)
names(cn) <- pull(yft_catch_2013_19, gear)

lfd <- lfd |>
  group_by(gear) |>
  mutate(Proportion=cn[gear]*fq/sum(fq))
```



```{r }
#| label: fig-cum_wt_prop_lfd
#| fig-cap: "Length frequency in relative numbers of fish for each gear based on the estimated total catch numbers."
ggplot(lfd, aes(x=Len, y=Proportion, fill=gear)) +
  labs(y="Relative catch numbers", x="Lenth (cm)", fill="Gear") +
  geom_col() +
  theme_classic()
```

## Priors

In data limited situations, information available on the stock biology is usually also limited and so information from other stocks and fisheries, along with local expert opinion, is required to compile the likely life history parameters that are necessary to interpret length frequency data. In general, length frequency data will not contain information to support estimates of natural mortality separate from fishing mortality, or information on growth. Therefore, in a Bayesian context, informative priors on key life history parameters will be required as input to the model.  

The information source for life history parameters used here is Fishbase (Froese and Pauly 2023). The life history of yellowfin tuna as a species (*Thunnus albacares*) is quite well researched, so there is reasonable confidence in values used for length-at-maturity, $L_\infty$ and length-weight parameters (@tbl-LH_priors). The prior on $M_K$ can also be proposed with reasonable confidence because of the values used in the main stock assessment. $M_K$ should generally be around 1.5 (Kenchington, 2014), and large departures from this would be suspect. The default growth variability prior (10% CV) is also used, but should not make a significant difference to the results. 

It should be noted that the official stock assessments do not use the von Bertalanffy growth model, but uses a variant, "Richard's model" (Maunder et al 2018), which allows two-stage growth that has been estimated to more closely match the true growth form. This may be a cause of some differences between the full stock assessment and this 'length data only' approach.   

```{r }
# Prepare data and priors
# ( from fishbase)
L50 <- 103 
L95 <- 120
Linf <- c(181.0454545, 5) # Fishbase reported values Linf 167-192.8: this is the mean
Galpha <- 100
a <- 0.00002459 
b <- 2.9667
Mk <- 1.714011976

# The data are rescaled to avoid numerical overflow. The sample size for purse
# seine is very large, whereas for other gears it may be small even where
# combined over 6 years. The effective sample size is likely much lower than the
# nominal anyway, so each sample is reduced by (the arbitrary) square root of
# its size.
lfd <- yft_lenfreq_2013_19 |>
  select(-Len) |>
  mutate(across(BB:`PS-LS`, ~ round(.x/sqrt(sum(.x))))) |>
  as.list()

ld1 <- blicc_dat(
  model_name = "All domed single selectivity",
  LLB = yft_lenfreq_2013_19$Len,
  fq = lfd,
  Linf = Linf,
  sel_fun=rep(4L, length(lfd)),
  Catch = yft_catch_2013_19$catch_n,
  gear_names = names(lfd),
  Mk=Mk,
  a=a,
  b=b,
  L50=L50,
  L95=L95
)
```



```{r}
#| label: tbl-LH_priors
#| tbl-cap: "Prior and fixed parameters values for non-gear related functions. Linf and Galpha are used in the growth model, Mk is the natural mortality and NB_phi is the scale parameter for the observation error. The fixed parameters are the length-weight exponent (b), and the logistic length-based maturity curve (L50, Ls)."
blicc_priors(ld1) |>
  filter( ! (Parameter %in% c("Fk", "Mode", "Left SD", "Right SD"))) |>
  select(Parameter:`Function Type`, Mu:SD) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 9) |>
  autofit()
```



# Catch Curve Model

The model structure is flexible, so the first task is to explore possible configurations that adequately explain the observations. One of the criteria for evaluation is to determine whether the final estimate of SPR is sensitive to alternative configurations. If it is not, then we do not need to worry too much about them and those (more complex) alternatives can be excluded from the final fit.  

The model applied here is for illustration only and the configurations are not exhaustively explored. For example, we would probably want to consider alternative data subsets such as by year or season, which would allow considering other possible effects such as recruitment or seasonal changes to selectivity but face complications in the length frequency data available for each gear.

The alternative model configurations are fitted using the Stan optimiser. This estimates the maximum posterior density. While this does not provide as much information on how well the model fits the data as the MCMC fit, it is considerably faster.  

## Single Selectivity Model

The first modelling attempt is to apply a single selectivity curve separately for each gear. For this purpose, the most flexible selectivity function, the double-sided normal is used (@fig-prior_lfd). This function has a single mode, so might not be expected to fit multi-modal frequencies well.  

```{r FitMultigear_Stan}
#| label: fig-prior_lfd
#| fig-cap: "Estimated length frequencies using prior parameters for single selectivity function for each gear."
plot_prior(ld1) +
  theme_classic()
```


```{r }
#| output: false
fit <- blicc_mpd(ld1)
res <- blicc_ref_pts(fit, ld1)
```


```{r}
#| label: fig-obsexp_1_lfd
#| fig-cap: "Expected length frequency data after the first fit."
plot_expected_frequency(res, gear=1:7) +
  theme_classic()
```

Sure enough, the model does not fit the data well (@fig-obsexp_1_lfd). With a single mode selectivity function, the selectivity model stretches between the modes to explain the higher or lower values. This overestimates the relative fishing mortality on these lengths between the modes which does not represent the data well. A better approach is needed that can account for more than one mode, so this approach is rejected. 

Multiple modes in length frequency data from a single gear is not necessarily unusual and can be observed in many fisheries. They seem particularly common in small scale fisheries. They can be due to a number of factors not all of which can be well modelled:

- modes may be the result of specific latent selectivity functions, where fish are caught in different ways due to their behaviour or shape. If these are static effects, they could be estimated as mixtures of selectivity functions. 

- sampling anomalies where modes are a result of the sampling noise or bias. For example, if a significant number of fish are sampled from a small set of trips or frequencies have been weighted by catches so a small sample is over-represented, these can form modes which are artefacts of the sampling and not representative of the overall gear selectivity or length composition of the population. Although selectivity functions might be used to explain such data, the selectivity model will remain unrepresentative of the overall fleet and its impact on the stock.

- temporary dynamics, such as high recruitment year classes observed in frequency data. In this case, methods such as ELEFAN might be used to track the modes and estimate growth and mortality. Attributing these to gear selectivity might still be possible if each catch in a sequence is modelled as a separate gear, but such an approach would need testing.

For yellowfin tuna, the main patterns have been observed consistently, so the first option - latent selectivities combined into single length frequency samples - probably applies. Sample sizes for some gears are still small despite these data having been combined over a long time period. As a result, selectivity functions may end up being chosen that only "explain away" observations that may not be representative of the true length frequency compositions, particularly as a result of sampling anomalies and mis-recording. If these do not make up a high proportion of the samples, they may not be an important contributor to fishing mortality, so their presence is convenient for fitting the model but still should not affect the final result.

A more flexible selectivity function required to explain multi-mode length frequencies may be obtained from mixtures of the basic selectivity functions (logistic and normal) rather than proposing anything new. This approach is used in `fishblicc` and is proposed below as an explanation for the different gears' length frequency.  

```{r}
sensitivities <- add_sensitivity(res, "Rejected")
```


## Shared-selectivity Mixture Model

For the shared selectivity model, the hypothesis is that the gear selectivities are made up of a mixture of logistic and normal-like selectivity functions. For example, both pole and line and purse seine take juvenile yellowfin associated with skipjack. The availability and capture of these juveniles might follow the same length-based selectivity function for both gears. A normal or double-sided normal selectivity function might be reasonably proposed based on the central limit theorem - fish are caught due to multiple factors linked to length that, when combined, peak at a particular length and decline on either side.  

Through hypothetical shared selectivity and subsequently explaining outliers that otherwise might be influential in the fit, a series of shared or independent functions were proposed and tested using trial and error. Based on the observations (@fig-length_frequency_data; @fig-obsexp_1_lfd), pole and line (BB) and purse seine FAD sets (PS-LS) share a peak around 50cm. We might assume both these gears encounter juveniles that are mixing with skipjack and therefore equally vulnerable to these gears. Handline (HD) and purse seine free-school (PS-FS) catches also seem to have minor peaks around 50cm and so likewise may have a similar selectivity shared component which can be included. This still leaves purse seine FAD sets with a minor but significant mode around 100cm which is not shared by any other gear. Another selectivity component is added around this point to explain this. The resulting selectivity functions, as demonstrated by the priors (@fig-prior_2_lfd), has sufficient flexibility to explain the observations without overfitting.

Using selectivity mixtures in this way does not discriminate between genuine selectivity mixture functions (fish are caught in different ways by the same gear) and mis-recording. For example, in the case of differences between purse seine free school and FAD sets, it is not always clear what a set is if it is conducted near a FAD or (natural) floating log. In this case the mixture may be partly an artefact of mis-recording or genuinely mixed set-types.  

In general, the actual selectivity functions should be as parsimonious as possible. Three functions (logistic, normal and single-sided normal[^2]) take 2 parameters only, whereas the double-sided normal takes three parameters. Discovering which functions are sufficient to explain the observations is a matter of trial and error.

[^2]: single-sided normal resembles the logistic but has a normal shape to the slope on the left of the mode. Its primary use is to test whether a selectivity might be dome-shaped as it is directly comparable with a double-sided normal: i.e. identical if the right-side slope parameter is fixed at zero. 

The mixture model fits these data much better than the single selectivity function model (@fig-obsexp_2_lfd). It is likely that the model no longer overestimates mortality between modes, so the selectivities and fishing mortality should be better estimated. This has resulted in a fall in the SPR to around 21% (@tbl-results_mix_fit) from around 60% in the single function model, so the effect of these changes to the model is not predictable. Given the much better fit, this model is put forward as the base model.

```{r}
#| warning: false

# Seven selectivity functions are setup including logistic (1),
# normal (2) and double-sided normal (4).
ld2 <- blicc_selfun(ld1,
                    sel_fun = c(2, 4, 2, 4, 1, 4, 4),
                    model_name = "Selectivity Mixture Model")

# The selectivity functions are then allocated among gears
ld2 <- blicc_gear_sel(ld2,
                      gear_sel = list(
                        BB = c(1, 2),
                        GI = c(4),
                        HD = c(5, 2),
                        LL = c(5),
                        OT = c(6),
                        `PS-FS` = c(7, 2),
                        `PS-LS` = c(1, 2, 3)
                      ))

# The priors are estimated for the non-shared selectivity functions
ld2 <- blip_selectivity(ld2)

# The prior hyper-parameters are then set manually for the other selectivity
# functions according to their hypothetical purpose
ld2 <- ld2 |>
  blip_set_sel(1, loc = 46) |>
  blip_set_sel(2, loc = 50) |>
  blip_set_sel(3, loc = 100) |>
  blip_set_sel(7, loc = 132, lslope = c(-4,-5))
# Where multiple functions are used for a single gear, mixture weight
ld2 <- blip_mix_wt(ld2, "BB", mix_wt = 0.2)
ld2 <- blip_mix_wt(ld2, "HD", mix_wt = 0.12)
ld2 <- blip_mix_wt(ld2, "PS-FS", mix_wt = 0.05)
ld2 <- blip_mix_wt(ld2, "PS-LS", mix_wt = c(0.2, 0.1))
```


```{r}
#| label: fig-prior_2_lfd
#| fig-cap: "Prior check: Estimated length frequency data using prior parameters for the mixture model."
plot_prior(ld2) +
  theme_classic()
```


```{r}
#| output: false
fit <- blicc_mpd(ld2)
res <- blicc_ref_pts(fit, ld2)
```


```{r}
#| label: fig-obsexp_2_lfd
#| fig-cap: "Estimated length frequency from the mixture model plotted against the data."
plot_expected_frequency(res) +
  theme_classic()
```

```{r}
#| label: tbl-results_mix_fit
#| tbl-cap: "Parameter estimates for the selectivity mixture model."
blicc_results(res) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 9) |>
  autofit()
```

It is a good idea to check the standardised residuals for the model to see whether there are significant outliers (@fig-residuals_mix). A problem with using the normal parametric selectivity function is that outliers in the distribution's tail can be very influential. These points can potentially be identified by large positive residuals. The residuals can be removed simply by adding additional selectivity functions at the cost of more parameters. Adding these selectivity functions may make little difference to the final mortality estimates because the weight on the function could be very low. Whether they are required or not can currently only be determined by trial and error. While more dispersed functions such as the gamma or lognormal may be more robust to this effect, they do not usually improve the fit much and can not be well justified on any theoretical grounds. Inclusion or exclusion of data in sensitivity analyses can often be justified where they are suspected to be incorrectly recorded or for some other reason.  

```{r}
#| label: fig-residuals_mix
#| fig-cap: "Standard residual plot by length bin separated for each gear."
plot_residuals(res) +
  facet_wrap(vars(Sgroup), scales="free_y") +
  theme_classic()
```


```{r}
#| include: false
sensitivities <- add_sensitivity(res, "Base model")
```

## Estimated $L_\infty$

The prior on the asymptotic $L_\infty$ is informative in the base model above. A less informative prior would allow this parameter to be estimated from the data if there is some support for it. The prior's influence can be easily reduced by increasing the standard deviation for the normal prior from 5cm to 15cm. The resulting model fit estimates $L_\infty$ much lower at 160cm (@tbl-results_Linf_fit) as opposed to 181cm given for this species from Fishbase.

```{r }
#| output: false
ld3 <- blip_Linf(ld2, Linf = c(181.0454545, 15), 
                 model_name="Allow Linf estimation")
fit <- blicc_mpd(ld3)
res <- blicc_ref_pts(fit, ld3)
```



```{r}
#| label: tbl-results_Linf_fit
#| tbl-cap: "Results for the model allowing Linf to be fitted."
blicc_results(res) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 8) |>
  autofit()
```

The model fits the data a little better because the log-probability (lp) is a little higher (@tbl-results_Linf_fit), but the estimated $L_\infty$ is too low to be realistic. The SPR estimate is highly sensitive to $L_\infty$ however. The implication is that the absence of larger fish in the length frequencies is either because higher mortality prevents them being in the population **or** they do not grow that big. The data does not support either hypothesis, so the prior on $L_\infty$ must be used choose between them. In this case, quite a lot is known about yellowfin, including how big they can get so the prior can be informative with reasonable confidence. If this was not known, both hypotheses could be combined as alternatives, or the informative prior might be chosen on basis that it is more precautionary.

```{r}
#| include: false
sensitivities <- add_sensitivity(res, "Rejected")
```

## Length-Inverse Natural Mortality

Lorenzen (2022) suggests that the default model for natural mortality in fish should be related to the inverse weight or length of the fish. This is the default used for most tuna stock assessments (Maunder et al. 2023) and is easily incorporated into this model by setting a reference length. A simple length-inverse model is implemented where natural mortality is inversely proportional to length:

$$ M_{L} = {M_{1} \over L} $$

Setting the reference length will set the natural mortality equal to the current prior natural mortality only at this length. The natural mortality will increase below this length and decrease above it according to the reciprocal relationship. In this formulation, no additional parameters are required to be fitted. A reference length of 140cm was chosen as being typical larger size taken by longline and handline. The results are not particularly sensitive to this choice, but in a full analysis more alternatives might be tested. 

The model fit improves, while the SPR estimate is not sensitive to this alternative model of natural mortality (@tbl-results_inverseL_M). This model is retained as the new base model because it fits a little better, it is the standard in tuna stock assessments, and is being used increasingly widely as a more accurate representation of the natural mortality process.  

```{r}
#| output: false
ld3 <- blip_Mk(ld2, lMk = c(ld1$polMkm, ld1$polMks),
                    ref_length = 140,
                    model_name = "Length-inverse M")
fit <- blicc_mpd(ld3)
res <- blicc_ref_pts(fit, ld3)
```



```{r}
#| label: tbl-results_inverseL_M
#| tbl-cap: "Results for the model with length-inverse M to be fitted."
blicc_results(res) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 8) |>
  autofit()
```


```{r}
#| include: false
sensitivities <- add_sensitivity(res, "New base")
```

## Dome-shaped Longline Selectivity

The current model assumes logistic selectivity function for longline and handline which catch the largest fish. This is the standard selectivity for most length-based methods. An alternative option is that the selectivity is dome-shaped (double-sided normal) which can be tested. Unlike the alternative models tested above, this requires an additional parameter to be estimated for the right-side downward slope of the selectivity function.  

With the additional parameter, there is only a insignificant improvement in the fit. The simple reason is that the longline selectivity mode is very close to the maximum size, so the downward slope of the selectivity function is only relevant to a very small proportion of fish and has little influence on the size composition of the catch. As a result, the additional parameter does not explain the observations much better and is redundant in this case.  Therefore this model is rejected.


```{r}
#| output: false
ld4 <- blicc_selfun(ld3, sel_fun = 4, sel_indx = 5, 
                    model_name = "Longline dome-shaped")

fit <- blicc_mpd(ld4)
res <- blicc_ref_pts(fit, ld4)
```


```{r}
#| tbl-cap: "Results for the model with length-inverse M to be fitted."
blicc_results(res) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 8) |>
  autofit()
```


```{r}
#| include: false
sensitivities <- add_sensitivity(res, "Rejected")
```


## Summary

In conclusion the final retained base model is the selectivity mixture model, with an informative $L_\infty$ prior and length-inverse M. Applying a separate single mode selectivity function to each gear resulted in a poor fit with a very low log-probability at the mpd mode (@tbl-sensitivities). Including the length-inverse M model component had only a small impact on SPR, but the slightly improved fit justified this model which is now widely used. This final model configuration is fitted using MCMC to estimate the uncertainty.


```{r}
#| echo: false
#| label: tbl-sensitivities
#| tbl-cap: "Summary of the alternative tested sensititivites, with the final base model fitted using MCMC. `lp` are the log-probabilities at the maximum posterior density points, which are almost directly comparible in terms of 'goodness-of-fit'. The final retained model is the 'Length-inverse M'."
sensitivities |>
  flextable() |>
  colformat_double(digits=2) |>
  fontsize(size = 9) |>
  align(j=7, align="center") |>
  autofit()
```


# MCMC Fit

The Markov chain Monte Carlo (MCMC) fit is carried out by Stan (mc-stan.org; Stan Development Team 2022). The model has been optimised as far as possible, but the calculation includes a numerical integration which requires a significant number of function evaluations and therefore the fit may take several hours to run dependent on the speed of your computer.  

Stan includes a number of diagnostics specific to its algorithm which indicate whether the MCMC has likely been successful or not. One of the most important diagnostics, particularly if mixtures are being used, are the "divergent" transitions. A very small number might be ignored, but in general the presence of these if reported indicates that the posterior has not been explored completely by the algorithm. This can be difficult to resolve and may indicate a poor fit of the model to the data. The problem may be best resolved by re-examining the selectivity functions to improve the fit to the observations. Otherwise, it is possible to increase the `adapt_delta` algorithm parameter closer to 1.0 (e.g. 0.9 or 0.95) to decrease the step size which may help. Note that the model already implements techniques such as non-centered parametrization, to improve stability in this regard.

In this case there were no divergences and MCMC diagnostics were generally good suggesting that the simulation had converged.

```{r }
#| eval: false
# Fitting takes a long time, so this is done once and the results are stored.
fit <- blicc_fit(ld3, ntarget = 2000, nwarmup = 2000, 
                 control=list(adapt_delta=0.8, max_treedepth=12))
res <- blicc_ref_pts(fit, ld3)
# Save fit for future use
save(fit, res, file=here("tmp.rda"))
```

```{r}
#| include: false
# load data
load(file=here("tmp.rda"))
```


```{r}
#| warnings: false
#| label: fig-pairs_plot
#| fig-cap: "Example pairs plot diagnostic from the MCMC fit for normalised scale parameters NBphi and Galpha, and the asymptotic length parameter Linf." 
# Full diagnostics are not provided in this document, but in a real assessment 
# the fit should be checked.
suppressWarnings(pairs(fit, pars=c("nNB_phi", "nLinf", "nGalpha")))
```

# Final Results

The MCMC gives estimates of uncertainty in the form of probability density functions for the values of interest. This includes the observations (length frequencies). Comparing the MCMC expected frequency with the observations implies a reasonable fit, with the majority of observations (bin counts) being within expected 80%-iles (@fig-obsexp_MCMC_lfd).

```{r}
#| label: fig-obsexp_MCMC_lfd
#| fig-cap: "Estimated length frequency from the mixture model MCMC fit plotted against the data."
plot_expected_frequency(res) +
  theme_classic()
```

Because MCMC implements a random search across the parameter volume, it is more likely to encounter "edge-cases" for the model when effectively different data are giving different signals. As has been noted above, the selectivity functions tend to highlight outliers with large standardised residuals. In this case, some simulations give large residuals particularly for gears "other" (OT), purse seine free schools (PS-FS) and pole and line (BB) (@fig-residuals_MCMC). These may be data anomalies or point to possible model improvements. In practice, explaining these small numbers of data points with selectivity functions should make little difference to the final results because they would have a very low weight, but they may improve accuracy and therefore reduce uncertainty in model estimates. 

```{r}
#| label: fig-residuals_MCMC
#| fig-cap: "Tandardised residuals taken from the MCMC fit. The high residuals indicate where outliers may be influential and some improvements in the selectivity functions may be possible."
plot_residuals(res) +
  facet_wrap(vars(Sgroup), scales="free_y") +
  theme_classic()
```

The MCMC results (@tbl-MCMC_results) are broadly in line with the maximum posterior results (@tbl-results_inverseL_M), but the parameter uncertainty and credible intervals are now properly estimated. Note that the raw fishing mortality (Fk) and selectivity (Sm) parameters are difficult to interpret in this form as they calculate the fishing mortality (per unit K) in each length bin. The actual fishing mortality fish are subjected to in each length bin will also depend on the length of time it takes to grow across the bin.


The selectivities indicate yellowfin is subject to a relatively high fishing mortality across all sizes (@fig-selectivity_MCMC). Selectivity of pole and line (BB) and purse seine on FADs (PS-LS) coincides to a large extent, whereas handline and longline are nearly identical since they share a common main selectivity function. The final spawning potential ratio (SPR) probability density suggests there is a high probability that the stock is overfished and it is highly likely the stock is below its target level (@fig-SPR).


```{r}
#| label: tbl-MCMC_results
#| tbl-cap: "MCMC estimates for the fitted parameters. See text for details."
blicc_results(res) |>
  flextable() |>
  colformat_double(digits=3) |>
  fontsize(size = 9) |>
  autofit()
```

```{r}
#| label: fig-selectivity_MCMC
#| fig-cap: "Estimated selectivities for each gear from the mixture model MCMC fit. The lines represent the MCMC average estimate and the ribbon the 80% credible interval. There is almost exact overlap between longline and handline producing the red ribbon."
plot_selectivity(res) +
  scale_fill_viridis_d(alpha=0.2, option="rocket") +
  theme_classic()
```

```{r}
#| label: fig-SPR
#| fig-cap: "Spawning potential ratio estimate from the mixture model MCMC fit."
plot_SPR_density(res) +
  theme_classic()
```
  

# Conclusions

This is a non-exhaustive illustrative assessment to fitting a non-standard length-based catch curve to length frequencies from multiple gears with complex inter-related selectivity. It is possible to construct selectivity mixtures that try to capture plausible hypotheses for the actual selectivities that gears may be applying. The model can be used to provide an estimate of stock status in these circumstances and identify important sensitivities to allow careful review. In data limited situations, the review process is important as almost invariably some subjective decisions have to be made where information is lacking.  

For yellowfin tuna in the example, the results suggest that the stock could well be overfished, although this cannot be fully determined from the length frequency alone.  The estimate of SB/SB~0~ for the full SS3 model in 2020 was 24-38% (80% CI), compared to the SPR estimate from this assessment of around 16-34%. There is considerable overlap in this case, although the length-based catch curve is more pessimistic which is not unreasonable in a data limited approach. As demonstrated above, other model choices would produce different estimates, so the SPR plausible range might be much wider when put through a full review process. The conclusion would still likely be to propose reductions in fishing mortality as the stock is likely to be overfished.  

A next step would be to simulate the population going forward under new management scenarios where the initial population state and gear selectivities are drawn from these model estimates. Such a model would be able to predict how catches and the population state might change with adjustments to fishing mortality and selectivity of each gear. The model would also predict the length composition data that would be expected from the altered fishery, so the model and effectiveness of the management changes are testable.  

# References

Froese, R., Pauly. D. Editors. 2023. FishBase. World Wide Web electronic publication. www.fishbase.org, version (06/2023).

Kenchington, T.J. 2014. Natural mortality estimators for information-limited fisheries. Fish and Fisheries, 15, 533–562  

Lorenzen, K. 2022. Size- and age-dependent natural mortality in fish populations: Biology, models, implications, and a generalized length-inverse mortality paradigm. https://doi.org/10.1016/j.fishres.2022.106454

Maunder, M.N., Deriso, R.B., Schaefer, K.M. et al. The growth cessation model: a growth model for species showing a near cessation in growth with application to bigeye tuna (*Thunnus obesus*). Mar Biol 165, 76 (2018). https://doi.org/10.1007/s00227-018-3336-9

Maunder, M.N., Hamel, O.S., Lee, H., Piner, K. R., Cope, J. M., Punt, A. E., Ianelli, J. N., Castillo-Jordan, C., Kapur, M.S. Methot, R. D. 2023. A review of estimation methods for natural mortality and their performance in the context of fishery stock assessment. Fisheries Research 257: 106489. https://doi.org/10.1016/j.fishres.2022.106489

Stan Development Team. 2022. Stan Modeling Language Users Guide and Reference Manual, 2.31. https://mc-stan.org

